{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec6d2a08",
   "metadata": {},
   "source": [
    "Universidad Nacional Experimental de Guayana\n",
    "\n",
    "Sistemas Distribuidos, Sección 1\n",
    "\n",
    "Bachilleres: Geruby Bermúdez Ricardo Muñoz\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc6aff8",
   "metadata": {},
   "source": [
    "# Práctica I. Parte II"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0480411",
   "metadata": {},
   "source": [
    "Para la realizacion de esta practica se necesitó tener instalado Scrapy, el cual es un framework de scraping y crawling de código abierto, escrito en Python. De esta forma, a importar scrapy podemos hacer uso de esta herramienta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca2cdfb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48188016",
   "metadata": {},
   "source": [
    "Luego de hacer unas busqueda en github sobre el personaje Leslie Lamport, obtenemos el siguiente enlace de un post:\n",
    "\n",
    "    https://github.com/joyoyoyoyoyo/lamport-logical-clocks-in-a-distributed-system\n",
    "    \n",
    "De dicho enlace obtendremos los siguientes elementos:\n",
    "\n",
    "    Nombre, autor, readme, idioma, lenguajes, fecha, vistas, estrellas,fork, paquetes, releases.\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae81e84",
   "metadata": {},
   "source": [
    "<img src=\"1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935dffbe",
   "metadata": {},
   "source": [
    "<img src=\"2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe0803c",
   "metadata": {},
   "source": [
    "Scrapy nos permite crear nuestra class QuoteSpider(scrapy.Spider), que a su vez permite definir cómo se raspará el sitio web seleccionado anteriormente, incluido cómo extraer datos estructurados de dicha pagina. \n",
    "\n",
    "-name es una cadena que define el nombre de nuestra araña\n",
    "\n",
    "-En start_urls se encuentra el enlace al post en GitHub del que haremos el raspado\n",
    "\n",
    "-parse es el método como función de devolución de llamada para las solicitudes.\n",
    "\n",
    "-resonse es la respuesta a los requerimientos sobre el sitio web que solicitadmos en nuestra araña \n",
    "\n",
    "-Con yield nos muestra el diccionario que contienen los datos extraídos de la página"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "74224754",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuoteSpider(scrapy.Spider):\n",
    "    name = 'gh'\n",
    "    start_urls = [\n",
    "        'https://github.com/joyoyoyoyoyo/lamport-logical-clocks-in-a-distributed-system/',\n",
    "    ]\n",
    "\n",
    "    def parse(self, response):\n",
    "        autor = response.xpath('//div/span/a/text()').extract_first()\n",
    "            \n",
    "        yield{'autor': autor}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b86a56",
   "metadata": {},
   "source": [
    "Para especificar la ruta de donde se deben obtener cada dato que requerimos del post en github se utilizo xpath junto con la inspeccion de elementos de la misma paguina web."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584222c7",
   "metadata": {},
   "source": [
    "<img src=\"3.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547849e6",
   "metadata": {},
   "source": [
    "Especificando la ruta con xpath quedaria de la siguiente manera:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd168c53",
   "metadata": {},
   "source": [
    "<img src=\"4.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ed4ca7",
   "metadata": {},
   "source": [
    "Para ingresar dicha ruta en nuestro proyecto, se escribiria la siguiente linea: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d763e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "autor = response.xpath('//div/span/a/text()').extract_first()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef092a5",
   "metadata": {},
   "source": [
    "Se realiza el mismo proceso para los demas elementos que queremos obtener de la paguina e igresamos cada ruta con la sintaxis que corresponde en nuestro proyecto, quedando nuestro codigo de la siguiente manera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7733fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "class QuoteSpider(scrapy.Spider):\n",
    "    name = 'gh'\n",
    "    start_urls = [\n",
    "        'https://github.com/joyoyoyoyoyo/lamport-logical-clocks-in-a-distributed-system/',\n",
    "    ]\n",
    "\n",
    "    def parse(self, response):\n",
    "        autor = response.xpath('//div/span/a/text()').extract_first()\n",
    "        titulo = response.xpath('//div/strong/a/text()').extract_first()\n",
    "        titulocarpeta1 = response.xpath('//div[2]/span/a/text()').extract_first()\n",
    "        tituloreadme = response.xpath('//div/h2/a/text()').extract_first()\n",
    "        infreadme = response.xpath('//div/div/pre/text()').extract_first()\n",
    "        star = response.xpath('//div[@class=\"mt-2\"]/a/strong/text()').extract_first()\n",
    "        view = response.xpath('//div[@class=\"mt-2\"][3]/a/strong/text()').extract_first()\n",
    "        forks = response.xpath('//div[@class=\"mt-2\"][4]/a/strong/text()').extract_first()\n",
    "        releases = response.xpath('//div[@class=\"text-small color-fg-muted\"]/text()').extract_first()\n",
    "        fecha = response.xpath('//relative-time[@class=\"no-wrap\"]/text()').extract_first()\n",
    "        tag1 = response.xpath('//a[@class=\"topic-tag topic-tag-link\"]/text()').extract_first()\n",
    "        tag2 = response.xpath('//div[@class=\"f6\"]/a[2]/text()').extract_first()\n",
    "            \n",
    "\n",
    "        yield{'autor': autor, 'titulo': titulo, 'titulo carpeta 1': titulocarpeta1, 'README.txt': tituloreadme,\n",
    "               'contenido README': infreadme, 'Star': star, 'view': view, 'forks': forks, 'releases': releases,\n",
    "               'fecha': fecha, 'Etiqueta 1': tag1, 'Etiqueta 2': tag2\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed683a2d",
   "metadata": {},
   "source": [
    "Al ejecutarlo en Visual Studio Code, con el comando:\n",
    "\n",
    "scrapy crawl gh -o pgh.csv\n",
    "\n",
    "Obtendremos los datos extraidos en un archivo csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c728d88a",
   "metadata": {},
   "source": [
    "# Diagramas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c518ec9f",
   "metadata": {},
   "source": [
    "La herramienta de raspado web de Python, Scrapy, utiliza un analizador de HTML para extraer información del código fuente HTML de una página. Esto se puede apreciar en el siguiente esquema:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f28758a",
   "metadata": {},
   "source": [
    "<img src=\"dg2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ba7c75",
   "metadata": {},
   "source": [
    "El proceso de raspado web con scrapy puede resumirse de forma general en una serie de 6 paso. En el primer paso se crea un projecto en scrapy (ejemplo: scrapy starproject tutorial), luego de que se cree la carpeta correspondiente, donde se encuentra nuestra araña, pasamos siguiente paso, que es crear el archivo .py donde estara nuestro codigo, (por ejemplo: scrpgh.py). Es importante que este archivo este dentro de la carpeta donde esta nuestra araña, de lo contrario no funcionara al momento de realizar el crawleo. \n",
    "Dentro de nuestro archivo scrpgh.py creamos la estrucutra de como se realizara la busqueda de datos y donde se guardaran estos datos, para ello abarcamos el siguiente paso, en el cual, mediante el lenguaje xpath o css e inspeccionanod el codigo de la pagina, encontramos las rutas donde se encuentran los datos que queremos extraer. \n",
    "Al tener la estructura de nuestro codigo lista con las rutas añadidas, rocedemos al ultimo paso que es el crawleo. Ejecutamos el comando scrapy crawl gh -o pgh.csv el cual realizara el crawleo y arrojara un archivo csv con los datos encontrados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7c6fc1",
   "metadata": {},
   "source": [
    "<img src=\"dg1.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c2d9a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
